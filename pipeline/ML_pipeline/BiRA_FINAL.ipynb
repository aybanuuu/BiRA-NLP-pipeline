{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zMY96yZOhcC0",
        "outputId": "c1a76ecd-8b8f-4618-ec54-b819083a6b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textgrid\n",
            "  Downloading TextGrid-1.6.1.tar.gz (9.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: textgrid\n",
            "  Building wheel for textgrid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for textgrid: filename=TextGrid-1.6.1-py3-none-any.whl size=10146 sha256=aa3f4ec0012903cd5aa9a18b8733648aadeacff8c264decc84de0c8c7bc5c689\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c5/96/5e43aa4c640995fbbb0b9a7b98e6007bfd777add3c7e56d70a\n",
            "Successfully built textgrid\n",
            "Installing collected packages: textgrid\n",
            "Successfully installed textgrid-1.6.1\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install textgrid\n",
        "!pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import gc\n",
        "import glob\n",
        "import zipfile\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import scipy.fftpack\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from itertools import zip_longest\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from textgrid import TextGrid\n",
        "from Levenshtein import distance as levenshtein\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.calibration import calibration_curve"
      ],
      "metadata": {
        "id": "O989p0gp-hUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zaAmwObhiTS"
      },
      "source": [
        "# DATA LOADING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwp6ZgZxhrvS"
      },
      "source": [
        "## AUDIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbDZYPZPhkGO",
        "outputId": "f97f50ee-6c48-4c14-cf23-c2bd98b066c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27FUs3n9ht0x"
      },
      "outputs": [],
      "source": [
        "zip_files = \"/content/drive/path/to/wav/zipfile\" # path to zip of wav files\n",
        "with zipfile.ZipFile(zip_files, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"audio\")\n",
        "\n",
        "wav_dir = \"/content/audio\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aKt6jhXUieoI"
      },
      "outputs": [],
      "source": [
        "wav_files = sorted(os.listdir(wav_dir))\n",
        "print(\"Extracted WAV files:\")\n",
        "for file in wav_files:\n",
        "    print(file)\n",
        "\n",
        "print(f\"Total number of WAV files: {len(wav_files)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcSpz4RjhxEt"
      },
      "source": [
        "## TEXTGRIDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnhUq5ruhzmK"
      },
      "outputs": [],
      "source": [
        "uploaded_textgrid = files.upload()\n",
        "print(\"Uploaded files:\", uploaded_textgrid.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahWPfwEfiHFE"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile('zipfile.zip', 'r') as zip_ref: # zip file of corresponding textgrids\n",
        "    zip_ref.extractall(\"textgrids\")\n",
        "\n",
        "textgrid_dir = \"/content/textgrids\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V-CngEuaimV3"
      },
      "outputs": [],
      "source": [
        "textgrid_files = sorted(os.listdir(textgrid_dir))\n",
        "print(\"Extracted TEXTGRID files:\")\n",
        "for file in textgrid_files:\n",
        "    print(file)\n",
        "\n",
        "print(f\"Total number of TEXTGRID files: {len(textgrid_files)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AewNe_z1i-Sk"
      },
      "source": [
        "# DATA PREPROCESSING AND FEATURE EXTRACTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fkg7fskLjBpm"
      },
      "outputs": [],
      "source": [
        "# phoneme to id dictionary\n",
        "phoneme_to_id = {\n",
        "    # vowels with stress symbols\n",
        "    \"AA0\": 0, \"AA1\": 1, \"AA2\": 2, \"AW0\": 3, \"AW1\": 4, \"AY0\": 5, \"AY1\": 6,\n",
        "    \"EH0\": 7, \"EH1\": 8, \"ER0\": 9, \"EY1\": 10,\n",
        "    \"IH1\": 11, \"IY0\": 12, \"IY1\": 13, \"IY2\": 14,\n",
        "    \"OW0\": 15, \"OW1\": 16, \"OW2\": 17, \"OY0\": 18, \"OY1\": 19,\n",
        "    \"UW0\": 20, \"UW1\": 21, \"UW2\": 22,\n",
        "    # Consonants (no stress markers)\n",
        "    \"B\": 23, \"D\": 24, \"F\": 25, \"G\": 26, \"H\": 27, \"JH\": 28, \"K\": 29, \"L\": 30, \"M\": 31, \"N\": 32,\n",
        "    \"NG\": 33, \"P\": 34, \"R\": 35, \"S\": 36, \"SH\": 37, \"T\": 38, \"V\": 39, \"W\": 40, \"Y\": 41, \"Z\": 42,\n",
        "    \"<BLANK>\": 43\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8toF5p-ljGgA"
      },
      "outputs": [],
      "source": [
        "#audio preprocessing by resampling all audio to 16kHz\n",
        "def preprocess_audio(file_path, target_sr=16000):\n",
        "    audio, sr = librosa.load(file_path, sr=None)\n",
        "    if sr != target_sr:\n",
        "        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
        "    return audio, target_sr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfYZJmfbGW4j"
      },
      "outputs": [],
      "source": [
        "def mel_filterbank(sr=16000, n_fft=512, n_mels=26, fmin=0, fmax=None):\n",
        "    if fmax is None:\n",
        "        fmax = sr // 2\n",
        "    mel_points = np.linspace(librosa.hz_to_mel(fmin),\n",
        "                             librosa.hz_to_mel(fmax),\n",
        "                             n_mels + 2)\n",
        "    hz_points = librosa.mel_to_hz(mel_points)\n",
        "    bin_points = np.floor((n_fft + 1) * hz_points / sr).astype(int)\n",
        "\n",
        "    filters = np.zeros((n_mels, int(n_fft // 2 + 1)))\n",
        "    for m in range(1, n_mels + 1):\n",
        "        f_m_minus = bin_points[m - 1]\n",
        "        f_m = bin_points[m]\n",
        "        f_m_plus = bin_points[m + 1]\n",
        "\n",
        "        for k in range(f_m_minus, f_m):\n",
        "            filters[m - 1, k] = (k - f_m_minus) / (f_m - f_m_minus)\n",
        "        for k in range(f_m, f_m_plus):\n",
        "            filters[m - 1, k] = (f_m_plus - k) / (f_m_plus - f_m)\n",
        "    return filters\n",
        "\n",
        "# mel filterbank frequency warping (VTLN)\n",
        "def apply_piecewise_warp(filters, warp_factor, pivot_freq, sr=16000, n_fft=512):\n",
        "    center_bin = int(np.floor((n_fft + 1) * pivot_freq / sr))\n",
        "\n",
        "    warped_filters = np.zeros_like(filters)\n",
        "    for i in range(filters.shape[0]):\n",
        "        orig_bins = np.arange(filters.shape[1])\n",
        "        warped_bins = np.where(\n",
        "            orig_bins <= center_bin,\n",
        "            orig_bins,\n",
        "            center_bin + (orig_bins - center_bin) * warp_factor\n",
        "        )\n",
        "        warped_filters[i] = np.interp(orig_bins, warped_bins, filters[i], left=0, right=0)\n",
        "    return warped_filters\n",
        "\n",
        "def extract_mfcc_vtln(signal, sr=16000, warp_factor=0.85, n_mfcc=13, n_mels=26, n_fft=512, hop_length=160):\n",
        "    pre_emphasis = 0.97\n",
        "    emphasized = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
        "    stft = librosa.stft(emphasized, n_fft=n_fft, hop_length=hop_length, win_length=400, window='hamming')\n",
        "    power_spec = np.abs(stft) ** 2\n",
        "    fb = mel_filterbank(sr=sr, n_fft=n_fft, n_mels=n_mels)\n",
        "    fb_warped = apply_piecewise_warp(fb, warp_factor, pivot_freq=1500, sr=sr, n_fft=n_fft)\n",
        "    mel_spec = np.dot(fb_warped, power_spec[:int(n_fft // 2 + 1), :])\n",
        "    mel_spec = mel_spec[1:-1, :]\n",
        "    log_mel_spec = librosa.power_to_db(mel_spec)\n",
        "    mfcc = scipy.fftpack.dct(log_mel_spec, axis=0, type=2, norm='ortho')[0:n_mfcc].T\n",
        "    return mfcc\n",
        "\n",
        "def extract_full_features(signal, sr=16000, warp_factor=0.85):\n",
        "    mfcc = extract_mfcc_vtln(signal, sr, warp_factor=warp_factor)\n",
        "    d_mfcc = librosa.feature.delta(mfcc)\n",
        "    dd_mfcc = librosa.feature.delta(mfcc, order=2)\n",
        "    f0, _, _ = librosa.pyin(signal, fmin=80, fmax=400, sr=sr, frame_length=1024, hop_length=160)\n",
        "    f0 = np.nan_to_num(f0, nan=0.0).reshape(-1, 1)\n",
        "    energy = librosa.feature.rms(y=signal, frame_length=400, hop_length=160).T\n",
        "\n",
        "    min_len = min(mfcc.shape[0], d_mfcc.shape[0], dd_mfcc.shape[0], f0.shape[0], energy.shape[0])\n",
        "\n",
        "    mfcc_stack = np.hstack([mfcc[:min_len], d_mfcc[:min_len], dd_mfcc[:min_len]])\n",
        "    mean = np.mean(mfcc_stack, axis=0)\n",
        "    std = np.std(mfcc_stack, axis=0) + 1e-10\n",
        "    mfcc_cmvn = (mfcc_stack - mean) / std\n",
        "\n",
        "    f0_part = f0[:min_len]\n",
        "    energy_part = energy[:min_len]\n",
        "\n",
        "    features = np.hstack([mfcc_cmvn, f0_part, energy_part])\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90vzzD7pjKN_"
      },
      "outputs": [],
      "source": [
        "# extract phoneme sequences from TextGrid files\n",
        "def extract_phoneme_sequence(textgrid_path, tier_name=\"phones\", skip_silence=True):\n",
        "    tg = TextGrid.fromFile(textgrid_path)\n",
        "    phoneme_tier = tg.getFirst(tier_name)\n",
        "\n",
        "    sequence = []\n",
        "    for interval in phoneme_tier:\n",
        "        label = interval.mark.strip().lower()\n",
        "        if skip_silence and label in [\"sil\", \"sp\", \"\"]:\n",
        "            continue\n",
        "        sequence.append(label.upper())\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KCodjG4SjMqk"
      },
      "outputs": [],
      "source": [
        "# feature extraction and storing features & labels in an npz file\n",
        "num_files = 0\n",
        "output_dir = '/content/train'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for wav_file in sorted(os.listdir(wav_dir)):\n",
        "  if wav_file.endswith('.wav'):\n",
        "    wav_path = os.path.join(wav_dir, wav_file)\n",
        "    textgrid_file = wav_file.replace('.wav', '.TextGrid')\n",
        "    textgrid_path = os.path.join(textgrid_dir, textgrid_file)\n",
        "\n",
        "    signal, sr = preprocess_audio(wav_path)\n",
        "    features = extract_full_features(signal, sr)\n",
        "\n",
        "    phonemes = extract_phoneme_sequence(textgrid_path)\n",
        "    phoneme_ids = [phoneme_to_id[p] for p in phonemes if p in phoneme_to_id]\n",
        "\n",
        "    np.savez(\n",
        "      os.path.join(output_dir, f\"{os.path.splitext(wav_file)[0]}.npz\"),\n",
        "      features=features,\n",
        "      labels=np.array(phoneme_ids)\n",
        "    )\n",
        "\n",
        "    del signal, features, phonemes, phoneme_ids\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    num_files += 1\n",
        "    print(f\"Processed {wav_file}, {num_files} files processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKETae0cOUMd"
      },
      "outputs": [],
      "source": [
        "shutil.make_archive(\"/content/drive/output/path/for/features\", 'zip', output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcXcmkdhjOCE"
      },
      "outputs": [],
      "source": [
        "class PhonemeDataset(Dataset):\n",
        "    def __init__(self, npz_file_paths):\n",
        "        self.paths = npz_file_paths\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = np.load(self.paths[idx])\n",
        "        features = data['features']\n",
        "        labels = data['labels']\n",
        "        return torch.tensor(features, dtype=torch.float32), torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqvsleSOjU0-"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    features, labels = zip(*batch)\n",
        "    feat_lengths = torch.tensor([f.shape[0] for f in features])\n",
        "    label_lengths = torch.tensor([len(l) for l in labels])\n",
        "\n",
        "    padded_feats = pad_sequence(features, batch_first=True)\n",
        "    concatenated_labels = torch.cat(labels)\n",
        "\n",
        "    return padded_feats, concatenated_labels, feat_lengths, label_lengths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohJUP5UUjhKg"
      },
      "source": [
        "# MODEL TRAINING AND TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06meGEirEMDy"
      },
      "outputs": [],
      "source": [
        "# load features here if downloaded to local storage\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/path/to/features.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "npz_files = sorted(os.listdir('/content/train'))\n",
        "print(\"Extracted NPZ files:\")\n",
        "for file in npz_files:\n",
        "    print(file)\n",
        "\n",
        "print(f\"Total number of WAV files: {len(npz_files)}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gf8Nicb9P5N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Tmvudw3di6_"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BzGM7ya_JhG"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "input_dim = 41\n",
        "hidden_dim = 128\n",
        "output_dim = len(phoneme_to_id)\n",
        "num_layers = 2\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "learning_rate = 1e-3\n",
        "dropout = 0.4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "QqXWa6DcbiaT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mmRgsjSjjNP"
      },
      "outputs": [],
      "source": [
        "class BiLSTM_CTC(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=num_layers, dropout=dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        x, _ = self.lstm(x, (h0, c0))\n",
        "        x = self.fc(x)\n",
        "        return self.log_softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba6FmwO8e8X4"
      },
      "outputs": [],
      "source": [
        "# convert probability outputs into phoneme sequence\n",
        "def ctc_greedy_decode(log_probs, blank=43, suppress_blanks=True):\n",
        "    probs = torch.exp(log_probs)\n",
        "    pred = torch.argmax(probs, dim=-1)\n",
        "\n",
        "    decoded = []\n",
        "    for b in range(pred.size(1)):\n",
        "        sequence = []\n",
        "        prev_token = -1\n",
        "        for t in range(pred.size(0)):\n",
        "            token = pred[t, b].item()\n",
        "            if token != blank:\n",
        "                if token != prev_token:\n",
        "                    sequence.append(token)\n",
        "            elif not suppress_blanks:\n",
        "                sequence.append(blank)\n",
        "            prev_token = token\n",
        "        decoded.append(sequence)\n",
        "    return decoded"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LR warmup\n",
        "class WarmupScheduler:\n",
        "    def __init__(self, optimizer, warmup_steps, base_lr):\n",
        "        self.optimizer    = optimizer\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.base_lr      = base_lr\n",
        "        self.step_count   = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.step_count += 1\n",
        "        if self.step_count <= self.warmup_steps:\n",
        "            scale = self.step_count / self.warmup_steps\n",
        "            for pg in self.optimizer.param_groups:\n",
        "                pg[\"lr\"] = self.base_lr * scale"
      ],
      "metadata": {
        "id": "xPrkMQbyqeJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "IRu8WdPQbmgn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiqSMyM7jl5J"
      },
      "outputs": [],
      "source": [
        "def train_ctc(model, dataloader, val_loader, num_epochs=num_epochs, lr=learning_rate, blank_id=43, patience=5, clip_norm=5.0):\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CTCLoss(blank=blank_id, zero_infinity=True)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "    warmup_scheduler = WarmupScheduler(optimizer, warmup_steps=500, base_lr=lr)\n",
        "    plateau_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=2, min_lr=1e-6)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "    best_model = None\n",
        "    accuracy_graph = []\n",
        "    trainloss_graph = []\n",
        "    validationsloss_graph = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = running_ctc = running_ent = 0\n",
        "        total_dist = 0\n",
        "        total_len = 0\n",
        "\n",
        "        for features, labels, feat_lens, label_lens in dataloader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            feat_lens, label_lens = feat_lens.to(device), label_lens.to(device)\n",
        "            log_probs = model(features).permute(1, 0, 2)\n",
        "            ctc_loss = criterion(log_probs, labels, feat_lens, label_lens)\n",
        "\n",
        "            probs = log_probs.exp()\n",
        "            entropy = -torch.sum(probs * log_probs, dim=-1)\n",
        "\n",
        "            mask = (\n",
        "                torch.arange(entropy.size(0), device=feat_lens.device)\n",
        "                .unsqueeze(1) < feat_lens.unsqueeze(0)\n",
        "            )\n",
        "\n",
        "            entropy_reg = (entropy * mask).sum() / mask.sum()\n",
        "            loss = ctc_loss - 0.0001 * entropy_reg\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
        "            optimizer.step()\n",
        "            warmup_scheduler.step()\n",
        "\n",
        "            running_ctc  += ctc_loss.item()\n",
        "            running_ent  += entropy_reg.item()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Accuracy (edit distance)\n",
        "            with torch.no_grad():\n",
        "                decoded_preds = ctc_greedy_decode(log_probs, blank=blank_id)\n",
        "                label_splits = torch.split(labels, label_lens.cpu().numpy().tolist())\n",
        "                for pred_seq, target_seq in zip(decoded_preds, label_splits):\n",
        "                    target_seq = target_seq.cpu().numpy().tolist()\n",
        "                    dist = levenshtein(pred_seq, target_seq)\n",
        "                    total_dist += dist\n",
        "                    total_len += len(target_seq)\n",
        "\n",
        "        acc = 100 * (1 - total_dist / total_len) if total_len > 0 else 0.0\n",
        "        PER = 100 * (total_dist / total_len) if total_len > 0 else 0.0\n",
        "        avg_train_loss = running_loss / len(dataloader)\n",
        "        avg_ctc_loss = running_ctc / len(dataloader)\n",
        "        avg_ent_loss = running_ent / len(dataloader)\n",
        "        accuracy_graph.append(acc)\n",
        "        trainloss_graph.append(avg_train_loss)\n",
        "        print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, CTC Loss: {avg_ctc_loss:.4f}, Entropy Loss: {avg_ent_loss:.4f} - Accuracy: {acc:.2f}%, PER: {PER:.2f}%\")\n",
        "\n",
        "        # ---- VALIDATION ----\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for features, labels, feat_lens, label_lens in val_loader:\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "                feat_lens, label_lens = feat_lens.to(device), label_lens.to(device)\n",
        "                log_probs = model(features).permute(1, 0, 2)\n",
        "                loss = criterion(log_probs, labels, feat_lens, label_lens)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                decoded_preds = ctc_greedy_decode(log_probs, blank=blank_id)\n",
        "                label_splits = torch.split(labels, label_lens.cpu().numpy().tolist())\n",
        "                for pred_seq, target_seq in zip(decoded_preds, label_splits):\n",
        "                    target_seq = target_seq.cpu().numpy().tolist()\n",
        "                    dist = levenshtein(pred_seq, target_seq)\n",
        "                    total_dist += dist\n",
        "                    total_len += len(target_seq)\n",
        "\n",
        "        PER = 100 * (total_dist / total_len) if total_len > 0 else 0.0\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        validationsloss_graph.append(avg_val_loss)\n",
        "        print(f\"Epoch {epoch+1} - Val Loss: {avg_val_loss:.4f} - PER: {PER:.2f}\")\n",
        "\n",
        "        plateau_scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Print LR\n",
        "        print(f\"Current Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # ---- EARLY STOPPING ----\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            epochs_without_improvement = 0\n",
        "            best_model = model.state_dict()\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            print(f\"No improvement for {epochs_without_improvement} epoch(s).\")\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # ---- Load Best Model ----\n",
        "    if best_model is not None:\n",
        "        model.load_state_dict(best_model)\n",
        "\n",
        "    # Plot training and validation accuracy and loss\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(accuracy_graph, label='Training Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Training Accuracy')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(trainloss_graph, label='Training Loss')\n",
        "    plt.grid(True)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Training Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(validationsloss_graph, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Validation Loss')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n2-f5j2j33A"
      },
      "outputs": [],
      "source": [
        "# organizes train-val-test splits based on phoneme distribution on all audio files\n",
        "\n",
        "# Load filenames from CSVs\n",
        "def extract_book_utt_key(filename):\n",
        "    parts = filename.split(\"-\")\n",
        "    book = parts[2]  # book number\n",
        "    utt = parts[3]   # utterance number\n",
        "    return f\"{book}-{utt}\"\n",
        "\n",
        "# Load original speaker splits\n",
        "train_keys = pd.read_csv(\"/content/train_files.csv\")[\"Filename\"].apply(extract_book_utt_key).tolist()\n",
        "val_keys   = pd.read_csv(\"/content/val_files.csv\")[\"Filename\"].apply(extract_book_utt_key).tolist()\n",
        "test_keys  = pd.read_csv(\"/content/test_files.csv\")[\"Filename\"].apply(extract_book_utt_key).tolist()\n",
        "\n",
        "npz_dir = \"/content/train\"\n",
        "train_npz, val_npz, test_npz = [], [], []\n",
        "\n",
        "for fname in os.listdir(npz_dir):\n",
        "    if fname.endswith(\".npz\"):\n",
        "        parts = fname.split(\"-\")\n",
        "        if len(parts) >= 4:\n",
        "            key = f\"{parts[2]}-{parts[3]}\"\n",
        "            if key in train_keys:\n",
        "                train_npz.append(fname)\n",
        "            elif key in val_keys:\n",
        "                val_npz.append(fname)\n",
        "            elif key in test_keys:\n",
        "                test_npz.append(fname)\n",
        "\n",
        "# Create full file paths\n",
        "train_files = [os.path.join(npz_dir, f) for f in train_npz]\n",
        "val_files   = [os.path.join(npz_dir, f) for f in val_npz]\n",
        "test_files  = [os.path.join(npz_dir, f) for f in test_npz]\n",
        "\n",
        "train_dataset = PhonemeDataset(train_files)\n",
        "test_dataset = PhonemeDataset(test_files)\n",
        "val_dataset = PhonemeDataset(val_files)\n",
        "\n",
        "print(len(train_files))\n",
        "print(len(test_files))\n",
        "print(len(val_files))\n",
        "\n",
        "for filez in sorted(val_files):\n",
        "    print(filez)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "LzVVeDIXtI4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BiLSTM_CTC(input_dim, hidden_dim, output_dim)"
      ],
      "metadata": {
        "id": "NJQE3-JQYU3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = train_ctc(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "HivJPr0PZ4su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Loop"
      ],
      "metadata": {
        "id": "IrCvEIB0bwIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# alignment with stress-aware matching\n",
        "def strip_stress(p):\n",
        "    return re.sub(r\"\\d\", \"\", p)\n",
        "\n",
        "def align_sequences(pred_seq, ref_seq):\n",
        "    n, m = len(ref_seq), len(pred_seq)\n",
        "    dp = np.zeros((n + 1, m + 1))\n",
        "    backtrace = [[None]*(m + 1) for _ in range(n + 1)]\n",
        "\n",
        "    # Initialize\n",
        "    for i in range(n + 1):\n",
        "        dp[i][0] = i\n",
        "        backtrace[i][0] = 'del'\n",
        "    for j in range(m + 1):\n",
        "        dp[0][j] = j\n",
        "        backtrace[0][j] = 'ins'\n",
        "    backtrace[0][0] = None\n",
        "\n",
        "    # Fill DP table\n",
        "    for i in range(1, n + 1):\n",
        "        for j in range(1, m + 1):\n",
        "            if ref_seq[i-1] == pred_seq[j-1]:\n",
        "                cost = 0\n",
        "                op = 'ok'\n",
        "            elif strip_stress(ref_seq[i-1]) == strip_stress(pred_seq[j-1]):\n",
        "                cost = 1\n",
        "                op = 'stress'\n",
        "            else:\n",
        "                cost = 1\n",
        "                op = 'sub'\n",
        "\n",
        "            options = [\n",
        "                (dp[i-1][j-1] + cost, op),\n",
        "                (dp[i-1][j] + 1, 'del'),\n",
        "                (dp[i][j-1] + 1, 'ins'),\n",
        "            ]\n",
        "            dp[i][j], backtrace[i][j] = min(options, key=lambda x: x[0])\n",
        "\n",
        "    # Backtrace\n",
        "    i, j = n, m\n",
        "    alignment = []\n",
        "    while i > 0 or j > 0:\n",
        "        op = backtrace[i][j]\n",
        "        if op == 'ok' or op == 'stress' or op == 'sub':\n",
        "            alignment.append((ref_seq[i-1], pred_seq[j-1], op))\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        elif op == 'del':\n",
        "            alignment.append((ref_seq[i-1], None, 'del'))\n",
        "            i -= 1\n",
        "        elif op == 'ins':\n",
        "            alignment.append((None, pred_seq[j-1], 'ins'))\n",
        "            j -= 1\n",
        "\n",
        "    alignment.reverse()\n",
        "    return alignment\n",
        "\n",
        "def visualize_alignment(ref_seq, pred_seq, phoneme_to_letter=None):\n",
        "    alignment = align_sequences(pred_seq, ref_seq)\n",
        "\n",
        "    ref_line = \"REF : \"\n",
        "    pred_line = \"PRED: \"\n",
        "    mark_line = \"      \"\n",
        "\n",
        "    for ref, pred, op in alignment:\n",
        "        # Map phonemes using dictionary\n",
        "        if phoneme_to_letter:\n",
        "            ref = phoneme_to_letter.get(ref, ref) if ref is not None else None\n",
        "            pred = phoneme_to_letter.get(pred, pred) if pred is not None else None\n",
        "\n",
        "        ref_token = f\"{ref:<5}\" if ref is not None else \"     \"\n",
        "        pred_token = f\"{pred:<5}\" if pred is not None else \"     \"\n",
        "\n",
        "        if op == \"ok\":\n",
        "            mark = \"‚úÖ\"\n",
        "        elif op == \"sub\":\n",
        "            mark = \"üîÑ\"\n",
        "        elif op == \"ins\":\n",
        "            mark = \"‚ûï\"\n",
        "        elif op == \"del\":\n",
        "            mark = \"‚ûñ\"\n",
        "        elif op == \"stress\":\n",
        "            mark = \"‚ö†\"\n",
        "        else:\n",
        "            mark = \"?\"\n",
        "\n",
        "        ref_line += ref_token\n",
        "        pred_line += pred_token\n",
        "        mark_line += f\"{mark:<5}\"\n",
        "\n",
        "    print(ref_line)\n",
        "    print(pred_line)\n",
        "    print(mark_line)"
      ],
      "metadata": {
        "id": "82XD3NBTEtss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um8DiM4pm_Zh"
      },
      "outputs": [],
      "source": [
        "def evaluate_ctc(model, dataloader, blank_id=43):\n",
        "    model.eval()\n",
        "    total_dist = 0\n",
        "    total_len = 0\n",
        "    all_preds = []\n",
        "    all_refs = []\n",
        "    id2phoneme = {v: k for k, v in phoneme_to_id.items()}\n",
        "    os.makedirs(\"/content/results\", exist_ok=True)\n",
        "    save_dir = \"/content/results\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, labels, feat_lens, label_lens in dataloader:\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "            feat_lens = feat_lens.to(device)\n",
        "            label_lens = label_lens.to(device)\n",
        "\n",
        "            log_probs = model(features)\n",
        "            log_probs = log_probs.permute(1, 0, 2)\n",
        "            decoded_preds = ctc_greedy_decode(log_probs, blank=blank_id)\n",
        "\n",
        "            label_splits = torch.split(labels, label_lens.cpu().numpy().tolist())\n",
        "\n",
        "            for pred_seq, ref_seq in zip(decoded_preds, label_splits):\n",
        "                ref_seq1 = ref_seq.cpu().numpy().tolist()\n",
        "                dist = levenshtein(pred_seq, ref_seq1)\n",
        "                total_dist += dist\n",
        "                total_len += len(ref_seq1)\n",
        "\n",
        "                ref_str = [id2phoneme[i.item()] for i in ref_seq]\n",
        "                pred_str = [id2phoneme[i] for i in pred_seq]\n",
        "\n",
        "                alignment = align_sequences(pred_str, ref_str)\n",
        "                for ref, pred, op in alignment:\n",
        "                    if ref is None or pred is None:\n",
        "                        continue\n",
        "                    all_refs.append(ref)\n",
        "                    all_preds.append(pred)\n",
        "\n",
        "\n",
        "    # Filter out <blank> pairs for metric evaluation\n",
        "    valid_pairs = [(p, r) for p, r in zip(all_preds, all_refs) if p != \"<BLANK>\" and r != \"<BLANK>\"]\n",
        "    filtered_preds = [p for p, r in valid_pairs]\n",
        "    filtered_refs = [r for p, r in valid_pairs]\n",
        "\n",
        "    # Create label set and encode to indices\n",
        "    phonemes = sorted(set(filtered_preds + filtered_refs))\n",
        "    phoneme_to_idx = {p: i for i, p in enumerate(phonemes)}\n",
        "    y_pred = [phoneme_to_idx[p] for p in filtered_preds]\n",
        "    y_true = [phoneme_to_idx[r] for r in filtered_refs]\n",
        "\n",
        "    # Classification report as dict and dataframe\n",
        "    report_dict = classification_report(y_true, y_pred, target_names=phonemes, zero_division=0, output_dict=True)\n",
        "    report_df = pd.DataFrame(report_dict).transpose()\n",
        "    phoneme_df = report_df.loc[phonemes]\n",
        "    report_df.to_csv(f\"{save_dir}/phoneme_classification_report.csv\")\n",
        "    print(f\"Saved classification report to {save_dir}/phoneme_classification_report.csv\")\n",
        "\n",
        "    # F1 score bar plot\n",
        "    f1_scores = report_df.loc[phonemes, \"f1-score\"]\n",
        "    sorted_scores = f1_scores.sort_values(ascending=True)\n",
        "    plt.figure(figsize=(10, len(sorted_scores) * 0.2))\n",
        "    sns.barplot(x=sorted_scores.values, y=sorted_scores.index, palette=\"viridis\")\n",
        "    plt.xlabel(\"F1 Score\")\n",
        "    plt.title(\"Phoneme F1 Scores\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{save_dir}/f1_score_plot.png\")\n",
        "    print(f\"Saved F1 score plot to {save_dir}/f1_score_plot.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fig, ax = plt.subplots(figsize=(15, 15))\n",
        "    disp = ConfusionMatrixDisplay(cm, display_labels=phonemes)\n",
        "    disp.plot(xticks_rotation=90, ax=ax, colorbar=True)\n",
        "    plt.title(\"Phoneme Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{save_dir}/confusion_matrix.png\")\n",
        "    print(f\"Saved confusion matrix image to {save_dir}/confusion_matrix.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Top Confused Phoneme Pairs\n",
        "    cm_df = pd.DataFrame(cm, index=phonemes, columns=phonemes)\n",
        "    confusions = cm_df.stack().reset_index()\n",
        "    confusions.columns = ['True', 'Pred', 'Count']\n",
        "    confusions = confusions[confusions['True'] != confusions['Pred']]\n",
        "    top_confusions = confusions.sort_values('Count', ascending=False).head(10)\n",
        "    print(\"Top Confused Phoneme Pairs:\\n\", top_confusions)\n",
        "    top_confusions.to_csv(f\"{save_dir}/top_confusions.csv\", index=False)\n",
        "    print(f\"Saved top confused phoneme pairs to {save_dir}/top_confusions.csv\")\n",
        "\n",
        "    # Least Predicted Phonemes\n",
        "    least_predicted = report_df.sort_values(\"support\").head(5)\n",
        "    least_predicted.to_csv(f\"{save_dir}/least_predicted_phonemes.csv\")\n",
        "    print(f\"Saved least predicted phonemes to {save_dir}/least_predicted_phonemes.csv\")\n",
        "\n",
        "    cm_df = pd.DataFrame(cm, index=phonemes, columns=phonemes)\n",
        "    fp = cm_df.sum(axis=0) - cm_df.values.diagonal()  # false positives per predicted label\n",
        "    fn = cm_df.sum(axis=1) - cm_df.values.diagonal()  # false negatives per reference label\n",
        "\n",
        "    # ‚Äî Top-10 most frequent phonemes\n",
        "    top10 = phoneme_df[\"support\"].sort_values(ascending=False).head(10).index.tolist()\n",
        "    cm_top10 = cm_df.loc[top10, top10]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    sns.heatmap(\n",
        "        cm_top10,\n",
        "        annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "        xticklabels=top10, yticklabels=top10,\n",
        "        cbar_kws={\"shrink\": 0.5}\n",
        "    )\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"True\")\n",
        "    ax.set_title(\"Confusion Matrix (Top-10 Frequent Phonemes)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{save_dir}/confusion_matrix_top10.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # ‚Äî 5 best / 5 worst by F1\n",
        "    f1 = phoneme_df[\"f1-score\"]\n",
        "    best5  = f1.sort_values(ascending=False).head(5)\n",
        "    worst5 = f1.sort_values(ascending=True).head(5)\n",
        "    f1_subset = pd.concat([best5, worst5])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    sns.barplot(x=f1_subset.values, y=f1_subset.index, orient=\"h\", ax=ax)\n",
        "    ax.set_xlabel(\"F1 Score\")\n",
        "    ax.set_ylabel(\"Phoneme\")\n",
        "    ax.set_title(\"Top 5 & Bottom 5 Phoneme F1 Scores\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{save_dir}/f1_scores_best5_worst5.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # False positive/negative stats\n",
        "    error_stats = pd.DataFrame({\n",
        "        \"False Positives\": fp,\n",
        "        \"False Negatives\": fn,\n",
        "        \"Support\": cm_df.sum(axis=1)\n",
        "    })\n",
        "    error_stats.to_csv(f\"{save_dir}/phoneme_false_pos_neg.csv\")\n",
        "    print(f\"Saved phoneme false positive/negative stats to {save_dir}/phoneme_false_pos_neg.csv\")\n",
        "\n",
        "    # Compute precision, recall, F1\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    print(\"Test Metrics:\")\n",
        "    print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f} | Accuracy sklearn: {accuracy:.2f}%\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=phonemes, zero_division=0))\n",
        "\n",
        "    acc = 100 * (1 - total_dist / total_len) if total_len > 0 else 0.0\n",
        "    print(f\"\\nAccuracy: {acc:.2f}%\")\n",
        "    print(f\"PER: {100 - acc:.2f}%\")\n",
        "    return acc\n",
        "\n",
        "evaluate_ctc(trained_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXYXdLFCqHWb"
      },
      "outputs": [],
      "source": [
        "torch.save(trained_model, \"/content/drive/path/to/model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "Lnd75mQIBtQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving weights\n",
        "torch.save(trained_model.state_dict(), \"bilstm_ctc_model_weights.pt\")"
      ],
      "metadata": {
        "id": "JKEvGG7TRU2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iQDA0d1Ia8V"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8zaAmwObhiTS",
        "cwp6ZgZxhrvS",
        "RcSpz4RjhxEt",
        "AewNe_z1i-Sk",
        "ohJUP5UUjhKg",
        "QqXWa6DcbiaT",
        "IRu8WdPQbmgn",
        "IrCvEIB0bwIG"
      ],
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}