{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This is a simple pipeline of the BiRA device's actual use case for pronunciation\n",
        "evaluation. Take note that audio inputs will come from the device's microphone, and\n",
        "the reference phonemes (denoted as variable 'ref') will come from the pronunciation lexicon.\n",
        "\n",
        "Users will select what Filipino word they would like to practice first. By selecting\n",
        "a word, the system will extract the reference phoneme sequence from the lexicon and ask\n",
        "the user to try pronouncing the word. It will record the users' voice, extract its\n",
        "features, and evaluate the pronunciation using the BiRA model.\n",
        "\n",
        "The actual device's evaluation has a scoring system and remarks based on the reference\n",
        "and predicted phoneme alignment in conjunction with the alignment result itself.\n",
        "'''"
      ],
      "metadata": {
        "id": "VAdQBdAwDgRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zMY96yZOhcC0",
        "outputId": "c1a76ecd-8b8f-4618-ec54-b819083a6b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textgrid\n",
            "  Downloading TextGrid-1.6.1.tar.gz (9.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: textgrid\n",
            "  Building wheel for textgrid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for textgrid: filename=TextGrid-1.6.1-py3-none-any.whl size=10146 sha256=aa3f4ec0012903cd5aa9a18b8733648aadeacff8c264decc84de0c8c7bc5c689\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c5/96/5e43aa4c640995fbbb0b9a7b98e6007bfd777add3c7e56d70a\n",
            "Successfully built textgrid\n",
            "Installing collected packages: textgrid\n",
            "Successfully installed textgrid-1.6.1\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import librosa\n",
        "import scipy.fftpack\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.serialization import add_safe_globals\n",
        "from Levenshtein import distance as levenshtein"
      ],
      "metadata": {
        "id": "O989p0gp-hUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AewNe_z1i-Sk"
      },
      "source": [
        "# LOAD MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fkg7fskLjBpm"
      },
      "outputs": [],
      "source": [
        "# phoneme to id dictionary\n",
        "phoneme_to_id = {\n",
        "    # vowels with stress symbols\n",
        "    \"AA0\": 0, \"AA1\": 1, \"AA2\": 2, \"AW0\": 3, \"AW1\": 4, \"AY0\": 5, \"AY1\": 6,\n",
        "    \"EH0\": 7, \"EH1\": 8, \"ER0\": 9, \"EY1\": 10,\n",
        "    \"IH1\": 11, \"IY0\": 12, \"IY1\": 13, \"IY2\": 14,\n",
        "    \"OW0\": 15, \"OW1\": 16, \"OW2\": 17, \"OY0\": 18, \"OY1\": 19,\n",
        "    \"UW0\": 20, \"UW1\": 21, \"UW2\": 22,\n",
        "    # Consonants (no stress markers)\n",
        "    \"B\": 23, \"D\": 24, \"F\": 25, \"G\": 26, \"H\": 27, \"JH\": 28, \"K\": 29, \"L\": 30, \"M\": 31, \"N\": 32,\n",
        "    \"NG\": 33, \"P\": 34, \"R\": 35, \"S\": 36, \"SH\": 37, \"T\": 38, \"V\": 39, \"W\": 40, \"Y\": 41, \"Z\": 42,\n",
        "    \"<BLANK>\": 43\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BzGM7ya_JhG"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "input_dim = 41\n",
        "hidden_dim = 128\n",
        "output_dim = len(phoneme_to_id)\n",
        "num_layers = 2\n",
        "dropout = 0.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Tmvudw3di6_"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mmRgsjSjjNP"
      },
      "outputs": [],
      "source": [
        "class BiLSTM_CTC(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=num_layers, dropout=dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        x, _ = self.lstm(x, (h0, c0))\n",
        "        x = self.fc(x)\n",
        "        return self.log_softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LifgJR8NqIv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90609022-2721-4079-b84c-ad5882ecac75",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM_CTC(\n",
              "  (lstm): LSTM(41, 128, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=44, bias=True)\n",
              "  (log_softmax): LogSoftmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# load model\n",
        "add_safe_globals({'BiLSTM_CTC': BiLSTM_CTC})\n",
        "\n",
        "trained_model = torch.load(\"/content/drive/path/to/model.pt\", weights_only=False)\n",
        "trained_model.to(device)\n",
        "trained_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohJUP5UUjhKg"
      },
      "source": [
        "# FUNCTIONS FOR EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfYZJmfbGW4j"
      },
      "outputs": [],
      "source": [
        "def mel_filterbank(sr=16000, n_fft=512, n_mels=26, fmin=0, fmax=None):\n",
        "    if fmax is None:\n",
        "        fmax = sr // 2\n",
        "    mel_points = np.linspace(librosa.hz_to_mel(fmin),\n",
        "                             librosa.hz_to_mel(fmax),\n",
        "                             n_mels + 2)\n",
        "    hz_points = librosa.mel_to_hz(mel_points)\n",
        "    bin_points = np.floor((n_fft + 1) * hz_points / sr).astype(int)\n",
        "\n",
        "    filters = np.zeros((n_mels, int(n_fft // 2 + 1)))\n",
        "    for m in range(1, n_mels + 1):\n",
        "        f_m_minus = bin_points[m - 1]\n",
        "        f_m = bin_points[m]\n",
        "        f_m_plus = bin_points[m + 1]\n",
        "\n",
        "        for k in range(f_m_minus, f_m):\n",
        "            filters[m - 1, k] = (k - f_m_minus) / (f_m - f_m_minus)\n",
        "        for k in range(f_m, f_m_plus):\n",
        "            filters[m - 1, k] = (f_m_plus - k) / (f_m_plus - f_m)\n",
        "    return filters\n",
        "\n",
        "# mel filterbank frequency warping (VTLN)\n",
        "def apply_piecewise_warp(filters, warp_factor, pivot_freq, sr=16000, n_fft=512):\n",
        "    center_bin = int(np.floor((n_fft + 1) * pivot_freq / sr))\n",
        "\n",
        "    warped_filters = np.zeros_like(filters)\n",
        "    for i in range(filters.shape[0]):\n",
        "        orig_bins = np.arange(filters.shape[1])\n",
        "        warped_bins = np.where(\n",
        "            orig_bins <= center_bin,\n",
        "            orig_bins,\n",
        "            center_bin + (orig_bins - center_bin) * warp_factor\n",
        "        )\n",
        "        warped_filters[i] = np.interp(orig_bins, warped_bins, filters[i], left=0, right=0)\n",
        "    return warped_filters\n",
        "\n",
        "def extract_mfcc_vtln(signal, sr=16000, warp_factor=0.85, n_mfcc=13, n_mels=26, n_fft=512, hop_length=160):\n",
        "    pre_emphasis = 0.97\n",
        "    emphasized = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
        "    stft = librosa.stft(emphasized, n_fft=n_fft, hop_length=hop_length, win_length=400, window='hamming')\n",
        "    power_spec = np.abs(stft) ** 2\n",
        "    fb = mel_filterbank(sr=sr, n_fft=n_fft, n_mels=n_mels)\n",
        "    fb_warped = apply_piecewise_warp(fb, warp_factor, pivot_freq=1500, sr=sr, n_fft=n_fft)\n",
        "    mel_spec = np.dot(fb_warped, power_spec[:int(n_fft // 2 + 1), :])\n",
        "    mel_spec = mel_spec[1:-1, :]\n",
        "    log_mel_spec = librosa.power_to_db(mel_spec)\n",
        "    mfcc = scipy.fftpack.dct(log_mel_spec, axis=0, type=2, norm='ortho')[0:n_mfcc].T\n",
        "    return mfcc\n",
        "\n",
        "def extract_full_features(signal, sr=16000, warp_factor=0.85):\n",
        "    mfcc = extract_mfcc_vtln(signal, sr, warp_factor=warp_factor)\n",
        "    d_mfcc = librosa.feature.delta(mfcc)\n",
        "    dd_mfcc = librosa.feature.delta(mfcc, order=2)\n",
        "    f0, _, _ = librosa.pyin(signal, fmin=80, fmax=400, sr=sr, frame_length=1024, hop_length=160)\n",
        "    f0 = np.nan_to_num(f0, nan=0.0).reshape(-1, 1)\n",
        "    energy = librosa.feature.rms(y=signal, frame_length=400, hop_length=160).T\n",
        "\n",
        "    min_len = min(mfcc.shape[0], d_mfcc.shape[0], dd_mfcc.shape[0], f0.shape[0], energy.shape[0])\n",
        "\n",
        "    mfcc_stack = np.hstack([mfcc[:min_len], d_mfcc[:min_len], dd_mfcc[:min_len]])\n",
        "    mean = np.mean(mfcc_stack, axis=0)\n",
        "    std = np.std(mfcc_stack, axis=0) + 1e-10\n",
        "    mfcc_cmvn = (mfcc_stack - mean) / std\n",
        "\n",
        "    f0_part = f0[:min_len]\n",
        "    energy_part = energy[:min_len]\n",
        "\n",
        "    features = np.hstack([mfcc_cmvn, f0_part, energy_part])\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba6FmwO8e8X4"
      },
      "outputs": [],
      "source": [
        "# convert probability outputs into phoneme sequence\n",
        "def ctc_greedy_decode(log_probs, blank=43, suppress_blanks=True):\n",
        "    probs = torch.exp(log_probs)\n",
        "    pred = torch.argmax(probs, dim=-1)\n",
        "\n",
        "    decoded = []\n",
        "    for b in range(pred.size(1)):\n",
        "        sequence = []\n",
        "        prev_token = -1\n",
        "        for t in range(pred.size(0)):\n",
        "            token = pred[t, b].item()\n",
        "            if token != blank:\n",
        "                if token != prev_token:\n",
        "                    sequence.append(token)\n",
        "            elif not suppress_blanks:\n",
        "                sequence.append(blank)\n",
        "            prev_token = token\n",
        "        decoded.append(sequence)\n",
        "    return decoded"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# alignment with stress-aware matching\n",
        "def strip_stress(p):\n",
        "    return re.sub(r\"\\d\", \"\", p)\n",
        "\n",
        "def align_sequences(pred_seq, ref_seq):\n",
        "    n, m = len(ref_seq), len(pred_seq)\n",
        "    dp = np.zeros((n + 1, m + 1))\n",
        "    backtrace = [[None]*(m + 1) for _ in range(n + 1)]\n",
        "\n",
        "    # Initialize\n",
        "    for i in range(n + 1):\n",
        "        dp[i][0] = i\n",
        "        backtrace[i][0] = 'del'\n",
        "    for j in range(m + 1):\n",
        "        dp[0][j] = j\n",
        "        backtrace[0][j] = 'ins'\n",
        "    backtrace[0][0] = None\n",
        "\n",
        "    # Fill DP table\n",
        "    for i in range(1, n + 1):\n",
        "        for j in range(1, m + 1):\n",
        "            if ref_seq[i-1] == pred_seq[j-1]:\n",
        "                cost = 0\n",
        "                op = 'ok'\n",
        "            elif strip_stress(ref_seq[i-1]) == strip_stress(pred_seq[j-1]):\n",
        "                cost = 1\n",
        "                op = 'stress'\n",
        "            else:\n",
        "                cost = 1\n",
        "                op = 'sub'\n",
        "\n",
        "            options = [\n",
        "                (dp[i-1][j-1] + cost, op),\n",
        "                (dp[i-1][j] + 1, 'del'),\n",
        "                (dp[i][j-1] + 1, 'ins'),\n",
        "            ]\n",
        "            dp[i][j], backtrace[i][j] = min(options, key=lambda x: x[0])\n",
        "\n",
        "    # Backtrace\n",
        "    i, j = n, m\n",
        "    alignment = []\n",
        "    while i > 0 or j > 0:\n",
        "        op = backtrace[i][j]\n",
        "        if op == 'ok' or op == 'stress' or op == 'sub':\n",
        "            alignment.append((ref_seq[i-1], pred_seq[j-1], op))\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        elif op == 'del':\n",
        "            alignment.append((ref_seq[i-1], None, 'del'))\n",
        "            i -= 1\n",
        "        elif op == 'ins':\n",
        "            alignment.append((None, pred_seq[j-1], 'ins'))\n",
        "            j -= 1\n",
        "\n",
        "    alignment.reverse()\n",
        "    return alignment\n",
        "\n",
        "def visualize_alignment(ref_seq, pred_seq, phoneme_to_letter=None):\n",
        "    alignment = align_sequences(pred_seq, ref_seq)\n",
        "\n",
        "    ref_line = \"REF : \"\n",
        "    pred_line = \"PRED: \"\n",
        "    mark_line = \"      \"\n",
        "\n",
        "    for ref, pred, op in alignment:\n",
        "        # Map phonemes using dictionary\n",
        "        if phoneme_to_letter:\n",
        "            ref = phoneme_to_letter.get(ref, ref) if ref is not None else None\n",
        "            pred = phoneme_to_letter.get(pred, pred) if pred is not None else None\n",
        "\n",
        "        ref_token = f\"{ref:<5}\" if ref is not None else \"     \"\n",
        "        pred_token = f\"{pred:<5}\" if pred is not None else \"     \"\n",
        "\n",
        "        if op == \"ok\":\n",
        "            mark = \"✅\"\n",
        "        elif op == \"sub\":\n",
        "            mark = \"🔄\"\n",
        "        elif op == \"ins\":\n",
        "            mark = \"➕\"\n",
        "        elif op == \"del\":\n",
        "            mark = \"➖\"\n",
        "        elif op == \"stress\":\n",
        "            mark = \"⚠\"\n",
        "        else:\n",
        "            mark = \"?\"\n",
        "\n",
        "        ref_line += ref_token\n",
        "        pred_line += pred_token\n",
        "        mark_line += f\"{mark:<5}\"\n",
        "\n",
        "    print(ref_line)\n",
        "    print(pred_line)\n",
        "    print(mark_line)"
      ],
      "metadata": {
        "id": "82XD3NBTEtss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-5ez5rtrsz1"
      },
      "source": [
        "# PRONUNCIATION EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9vck8xu97Dt"
      },
      "outputs": [],
      "source": [
        "# after obtaining user's voice record, load audio\n",
        "signal, sr = librosa.load(\"/content/mais.wav\", sr=16000)\n",
        "#signal.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1_2vegj-HjQ",
        "outputId": "682c5e83-8615-402b-e9a6-a2ae820ab585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REF : M    A0        I1   S    \n",
            "PRED: M    A0   I0   E1   S    \n",
            "      ✅    ✅    ➕    🔄    ✅    \n"
          ]
        }
      ],
      "source": [
        "# feature extraction of voice record\n",
        "features = extract_full_features(signal, sr=16000)\n",
        "features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "# model will now evaluate the pronunciation\n",
        "with torch.no_grad():\n",
        "    log_probs = trained_model(features_tensor)\n",
        "    decoded = ctc_greedy_decode(log_probs.permute(1, 0, 2), blank=43)\n",
        "\n",
        "phoneme_to_letter = {\n",
        "    \"AA0\": \"A0\", \"AA1\": \"A1\", \"AA2\": \"A2\", \"EH0\": \"E0\", \"EH1\": \"E1\", \"IH1\": \"I1\",\n",
        "    \"IY0\": \"I0\", \"IY1\": \"I1\", \"IY2\": \"I2\", \"UW0\": \"U0\", \"UW1\": \"U1\", \"UW2\": \"U2\"\n",
        "}\n",
        "\n",
        "id2phoneme = {v: k for k, v in phoneme_to_id.items()}\n",
        "# Reference transcription (based on the pronunciation lexicon)\n",
        "ref = [phoneme_to_id[p] for p in [\"M\", \"AA0\", \"IY1\", \"S\"]] # mais\n",
        "#ref = [phoneme_to_id[p] for p in [\"K\", \"AY1\", \"G\", \"AA0\", \"N\", \"D\", \"AA1\", \"N\", \"AA1\", \"NG\", \"H\", \"IY1\", \"K\", \"AW0\", \"M\", \"OW1\"]] # kay ganda ng hikaw mo\n",
        "#ref = [phoneme_to_id[p] for p in [\"S\", \"UW1\", \"S\", \"IY0\"]] # susi\n",
        "#ref = [phoneme_to_id[p] for p in [\"AA0\", \"M\", \"P\", \"AA0\", \"L\", \"AA0\", \"Y\", \"AA1\"]] # ampalaya\n",
        "#ref = [phoneme_to_id[p] for p in [\"OW1\", \"K\", \"R\", \"AA0\"]] # okra\n",
        "\n",
        "# visualizing alignment\n",
        "pred_str = [id2phoneme[i] for i in decoded[0]]\n",
        "ref_str = [id2phoneme[i] for i in ref]\n",
        "visualize_alignment(ref_str, pred_str, phoneme_to_letter)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "AewNe_z1i-Sk",
        "ohJUP5UUjhKg",
        "_-5ez5rtrsz1"
      ],
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}